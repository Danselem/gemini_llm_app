# ğŸ”® Google Gemini LLM App

A demonstration project for building LLM applications using Google's **Gemini models**, with **LangChain**, **ChromaDB**, and **Retrieval-Augmented Generation (RAG)**. This app shows how to load documents, embed them, store in a vector DB, and answer queries based on context.

## ğŸ‘¤ Author

- **Daniel Egbo** â€“ [@Danselem](https://github.com/Danselem)

---

## ğŸ“¦ Features

- âœ… Integration with **Google Gemini** (e.g., `gemini-1.5-flash`)
- ğŸ“„ PDF ingestion and preprocessing
- âœ‚ï¸ Intelligent document chunking with LangChain
- ğŸ§  Vector embeddings using Gemini
- ğŸ—ƒï¸ ChromaDB as the vector store
- ğŸ” Semantic search for relevant document chunks
- ğŸ’¬ Question-answering using a RAG pipeline
- ğŸ§ª Includes example usage with IRS documents

---

## ğŸ“ Project Structure

```text
gemini_llm_app/
â”œâ”€â”€ config
â”‚   â””â”€â”€ logging_config.yaml
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ embeddings
â”‚   â”œâ”€â”€ image
â”‚   â”‚   â””â”€â”€ outputs
â”‚   â”œâ”€â”€ outputs
â”‚   â”‚   â”œâ”€â”€ safety_output.txt
â”‚   â”‚   â””â”€â”€ think_output.txt
â”‚   â”œâ”€â”€ pdfs
â”‚   â”‚   â””â”€â”€ p554.pdf
â”‚   â””â”€â”€ prompts
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ examples
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ funccall.py
â”‚   â”œâ”€â”€ ima_gen.py
â”‚   â”œâ”€â”€ psumm.py
â”‚   â”œâ”€â”€ psumm2.py
â”‚   â”œâ”€â”€ quick.py
â”‚   â”œâ”€â”€ rag
â”‚   â”‚   â””â”€â”€ app.py
â”‚   â”œâ”€â”€ safety.py
â”‚   â”œâ”€â”€ search.py
â”‚   â”œâ”€â”€ struct.py
â”‚   â””â”€â”€ think.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ embeddings
â”‚   â”‚   â””â”€â”€ gemini_embedding.py
â”‚   â”œâ”€â”€ handlers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ error_handler.cpython-311.pyc
â”‚   â”‚   â””â”€â”€ error_handler.py
â”‚   â”œâ”€â”€ llm
â”‚   â”‚   â”œâ”€â”€ gemini_client.py
â”‚   â”‚   â”œâ”€â”€ lang_gemini.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ prompt_engineering
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ templates.py
â”‚   â””â”€â”€ utils
â”‚       â”œâ”€â”€ doc_split.py
â”‚       â”œâ”€â”€ download_file.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ rate_limiter.py
â”œâ”€â”€ storage
â”‚   â””â”€â”€ chroma
â”‚       â””â”€â”€ chroma.sqlite3
â””â”€â”€ uv.lock
```
---
## ğŸš€ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/Danselem/gemini_llm_app.git
cd gemini_llm_app
```
### 2. Create and Activate a Virtual Environment
This project uses the [`uv`](https://docs.astral.sh/uv/) project manager and [`make`](https://www.gnu.org/software/make/) tools.
```bash
uv venv --python 3.11
source .venv/bin/activate
```
### 3. Install Dependencies
```bash
uv pip install --all-extras --requirement backend/pyproject.toml
```
or
```bash
make install
```
---
## ğŸ” Environment Variables
Create a `.env` file at the project root and fill the environment variables with `make env`.
```env
GOOGLE_API_KEY=your_gemini_api_key
MULTIMODAL_MODEL=gemini-1.5-flash
```
---
## â–¶ï¸ Run the App
There are multiple examples in the `examples` directory for you to get started with, e.g.:

```bash
python -m examples.psumm
```
or
```bash
make psumm
```
The example code summarises a pdf file. There are multiple examples in the `examples` directory. You can also check out the `Makefile` to see other examples.

---

## âš™ï¸ How It Works
### 1. Download & Parse PDF
The PDF is downloaded from a URL if it's not already in `data/pdfs`.

### 2. Text Splitting
The PDF is split into overlapping chunks using `RecursiveCharacterTextSplitter` from LangChain.

### 3. Embeddings
Each chunk is converted into a vector using Google Gemini Embedding (via LangChain wrapper).

### 4. Vector Store
The chunks and their embeddings are stored in a local ChromaDB collection.

### 5. Semantic Search
When a user asks a question, the app retrieves relevant chunks using vector similarity search.

### 6. RAG Response
The relevant chunks are passed to Gemini to answer the question contextually.

---
## ğŸ“š Tech Stack
* LangChain

* ChromaDB

* Google Gemini API

* Poppler

* Python 3.11+

---

## ğŸ“„ License
MIT License. See the [LICENSE](LICENSE) file.

---

## ğŸ™ Acknowledgements
* Google for releasing the Gemini family of models

* LangChain community for open-source tools

* ChromaDB team for fast and easy vector storage

---

## ğŸ’¡ Ideas for Extension
* ğŸ”§ Add a simple web UI with Gradio or Streamlit for the RAG application.

* ğŸ“ Ingest multiple documents and support multi-source QA.

* ğŸ§  Add caching to avoid re-embedding on re-runs.

* ğŸ“Š Integrate telemetry/tracing and observability with `Open Inference and Phoenix.